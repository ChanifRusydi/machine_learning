{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "success = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https:\\ultralytics.com\\images\\bus.jpg locally at bus.jpg\n",
      "image 1/1 C:\\Users\\User\\Documents\\machine_learning\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 25.0ms\n",
      "Speed: 6.0ms preprocess, 25.0ms inference, 75.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "results= model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 4 persons, 1 bus, 1 stop sign, 56.0ms\n",
      "Speed: 190.3ms preprocess, 56.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image= Image.open(\"bus.jpg\")\n",
    "results= model.predict(source=image, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!yolo mode=predict model=yolov8n.pt source=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "       \n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        \n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "    \n",
    "        self.box_annotator = sv.BoxAnnotator(sv.ColorPalette.default(), thickness=3, text_thickness=3, text_scale=1.5)\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "       \n",
    "        # model = YOLO(\"yolov8m.pt\")  # load a pretrained YOLOv8n model\n",
    "        model = YOLO(\"yolov8n.pt\")\n",
    "        model.fuse()\n",
    "    \n",
    "        return model\n",
    "\n",
    "\n",
    "    def predict(self, frame):\n",
    "       \n",
    "        results = self.model(frame)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        \n",
    "        xyxys = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "         # Extract detections for person class\n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            # class_id = boxes.cls[0]\n",
    "            # conf = boxes.conf[0]\n",
    "            # xyxy = boxes.xyxy[0]\n",
    "\n",
    "            if class_ids == 0.0:\n",
    "          \n",
    "              xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "              confidences.append(result.boxes.conf.cpu().numpy())\n",
    "              class_ids.append(result.boxes.cls.cpu().numpy().astype(int))\n",
    "            \n",
    "        \n",
    "        # Setup detections for visualization\n",
    "        detections = sv.Detections(\n",
    "                    xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "                    confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "                    class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n",
    "                    )\n",
    "        \n",
    "    \n",
    "        # Format custom labels\n",
    "        self.labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "        for _, confidence, class_id, tracker_id\n",
    "        in detections]\n",
    "        \n",
    "        # Annotate and display frame\n",
    "        frame = self.box_annotator.annotate(scene=frame, detections=detections, labels=self.labels)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __call__(self):\n",
    "\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "      \n",
    "        while True:\n",
    "          \n",
    "            start_time = time()\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "            \n",
    "            results = self.predict(frame)\n",
    "            frame = self.plot_bboxes(results, frame)\n",
    "            \n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time - start_time, 2)\n",
    "             \n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "            \n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    " \n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    \n",
    "detector = ObjectDetection(capture_index=0)\n",
    "detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/chanifrusydi/machine_learning/image1_50_left.jpg: 640x480 2 cars, 1 fire hydrant, 65.3ms\n",
      "Speed: 3.5ms preprocess, 65.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "keys: ['boxes']\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'bus', 5: 'train', 6: 'truck', 7: 'traffic light', 8: 'fire hydrant', 9: 'stop sign', 10: 'cat', 11: 'dog'}\n",
      "orig_img: array([[[114, 137, 122],\n",
      "        [ 96, 119, 104],\n",
      "        [ 74,  95,  80],\n",
      "        ...,\n",
      "        [103, 152, 120],\n",
      "        [ 93, 148, 115],\n",
      "        [ 79, 140, 106]],\n",
      "\n",
      "       [[ 90, 111,  96],\n",
      "        [ 78,  99,  84],\n",
      "        [ 86, 107,  92],\n",
      "        ...,\n",
      "        [104, 154, 122],\n",
      "        [ 91, 148, 115],\n",
      "        [ 76, 140, 105]],\n",
      "\n",
      "       [[ 89, 108,  91],\n",
      "        [ 90, 109,  92],\n",
      "        [ 82, 100,  83],\n",
      "        ...,\n",
      "        [117, 170, 137],\n",
      "        [101, 161, 127],\n",
      "        [ 82, 152, 116]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[137, 142, 145],\n",
      "        [139, 144, 147],\n",
      "        [134, 140, 145],\n",
      "        ...,\n",
      "        [147, 162, 171],\n",
      "        [164, 179, 188],\n",
      "        [165, 180, 189]],\n",
      "\n",
      "       [[139, 144, 147],\n",
      "        [134, 139, 142],\n",
      "        [131, 137, 142],\n",
      "        ...,\n",
      "        [154, 173, 181],\n",
      "        [168, 187, 195],\n",
      "        [180, 199, 207]],\n",
      "\n",
      "       [[140, 145, 148],\n",
      "        [132, 137, 140],\n",
      "        [131, 137, 142],\n",
      "        ...,\n",
      "        [189, 209, 220],\n",
      "        [190, 210, 221],\n",
      "        [196, 216, 227]]], dtype=uint8)\n",
      "orig_shape: (991, 743)\n",
      "path: '/Users/chanifrusydi/machine_learning/image1_50_left.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 3.5381317138671875, 'inference': 65.34290313720703, 'postprocess': 0.7028579711914062}\n"
     ]
    }
   ],
   "source": [
    "# import ultralytics \n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "image = cv2.imread('image1_50_left.jpg')\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model.predict('image1_50_left.jpg')\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "box = result.boxes[2]\n",
    "print(len(box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.) tensor([183.4177, 659.8679, 208.6181, 715.3016]) tensor(0.5735)\n"
     ]
    }
   ],
   "source": [
    "print(box.cls[0], box.xyxy[0], box.conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id, coordinates, confidence = box.cls[0].tolist(), box.xyxy[0].tolist(), box.conf[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0,\n",
       " [183.4176788330078, 659.8678588867188, 208.61813354492188, 715.3016357421875],\n",
       " 0.5734856128692627)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id, coordinates, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([183, 660, 209, 715], 'fire hydrant')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rounding the coordinates an== convert class id into class name\n",
    "coordinates = [round(point) for point in coordinates]\n",
    "class_name = result.names[box.cls[0].item()]\n",
    "coordinates, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'bus', 5: 'train', 6: 'truck', 7: 'traffic light', 8: 'fire hydrant', 9: 'stop sign', 10: 'cat', 11: 'dog'}\n"
     ]
    }
   ],
   "source": [
    "print(result.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id, coordinate, confidence = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "boxes: tensor([[182.7726, 662.9521, 208.8178, 714.5112,   0.9284,  10.0000]])\n",
       "cls: tensor([10.])\n",
       "conf: tensor([0.9284])\n",
       "data: tensor([[182.7726, 662.9521, 208.8178, 714.5112,   0.9284,  10.0000]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (991, 743)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[195.7952, 688.7317,  26.0452,  51.5591]])\n",
       "xywhn: tensor([[0.2635, 0.6950, 0.0351, 0.0520]])\n",
       "xyxy: tensor([[182.7726, 662.9521, 208.8178, 714.5112]])\n",
       "xyxyn: tensor([[0.2460, 0.6690, 0.2810, 0.7210]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box = result.boxes[0]\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres =  0.6\n",
    "        iou_thres = 0.45,\n",
    "        classes = None,\n",
    "        agnostic = False,\n",
    "        multi_label = False,\n",
    "        labels = ().\n",
    "        max_det = 300,\n",
    "        nc = 12,\n",
    "        max_time_img = 0.1,\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(self, preds, img, original_images):\n",
    "    preds = non_max_suppression(preds, conf_thres=0.6, iou_thres= 0.45, agnostic=False, max_det =3000, classes=None)\n",
    "    results=[]\n",
    "    for i, pred in enumerate(preds):\n",
    "        original_images = original_images[i] if isinstance(original_images, list) else original_images\n",
    "        if not isinstance(original_images, torch.Tensor):\n",
    "            original_images = torch.from_numpy(original_images).to(self.device)\n",
    "        path = self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
