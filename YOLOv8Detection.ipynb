{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "success = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https:\\ultralytics.com\\images\\bus.jpg locally at bus.jpg\n",
      "image 1/1 C:\\Users\\User\\Documents\\machine_learning\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 25.0ms\n",
      "Speed: 6.0ms preprocess, 25.0ms inference, 75.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "results= model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 4 persons, 1 bus, 1 stop sign, 56.0ms\n",
      "Speed: 190.3ms preprocess, 56.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image= Image.open(\"bus.jpg\")\n",
    "results= model.predict(source=image, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!yolo mode=predict model=yolov8n.pt source=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 31.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 116\u001b[0m\n\u001b[0;32m    111\u001b[0m         cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    115\u001b[0m detector \u001b[39m=\u001b[39m ObjectDetection(capture_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m detector()\n",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m, in \u001b[0;36mObjectDetection.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39massert\u001b[39;00m ret\n\u001b[0;32m     96\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(frame)\n\u001b[1;32m---> 97\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplot_bboxes(results, frame)\n\u001b[0;32m     99\u001b[0m end_time \u001b[39m=\u001b[39m time()\n\u001b[0;32m    100\u001b[0m fps \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mround(end_time \u001b[39m-\u001b[39m start_time, \u001b[39m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mObjectDetection.plot_bboxes\u001b[1;34m(self, results, frame)\u001b[0m\n\u001b[0;32m     63\u001b[0m detections \u001b[39m=\u001b[39m sv\u001b[39m.\u001b[39mDetections(\n\u001b[0;32m     64\u001b[0m             xyxy\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mxyxy\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     65\u001b[0m             confidence\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mconf\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     66\u001b[0m             class_id\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mcls\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m),\n\u001b[0;32m     67\u001b[0m             )\n\u001b[0;32m     70\u001b[0m \u001b[39m# Format custom labels\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLASS_NAMES_DICT[class_id]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconfidence\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m _, confidence, class_id, tracker_id\n\u001b[0;32m     73\u001b[0m \u001b[39min\u001b[39;00m detections]\n\u001b[0;32m     75\u001b[0m \u001b[39m# Annotate and display frame\u001b[39;00m\n\u001b[0;32m     76\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_annotator\u001b[39m.\u001b[39mannotate(scene\u001b[39m=\u001b[39mframe, detections\u001b[39m=\u001b[39mdetections, labels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels)\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m detections \u001b[39m=\u001b[39m sv\u001b[39m.\u001b[39mDetections(\n\u001b[0;32m     64\u001b[0m             xyxy\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mxyxy\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     65\u001b[0m             confidence\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mconf\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     66\u001b[0m             class_id\u001b[39m=\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mcls\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m),\n\u001b[0;32m     67\u001b[0m             )\n\u001b[0;32m     70\u001b[0m \u001b[39m# Format custom labels\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLASS_NAMES_DICT[class_id]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconfidence\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m _, confidence, class_id, tracker_id\n\u001b[0;32m     73\u001b[0m \u001b[39min\u001b[39;00m detections]\n\u001b[0;32m     75\u001b[0m \u001b[39m# Annotate and display frame\u001b[39;00m\n\u001b[0;32m     76\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_annotator\u001b[39m.\u001b[39mannotate(scene\u001b[39m=\u001b[39mframe, detections\u001b[39m=\u001b[39mdetections, labels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "       \n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        \n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "    \n",
    "        self.box_annotator = sv.BoxAnnotator(sv.ColorPalette.default(), thickness=3, text_thickness=3, text_scale=1.5)\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "       \n",
    "        # model = YOLO(\"yolov8m.pt\")  # load a pretrained YOLOv8n model\n",
    "        model = YOLO(\"yolov8n.pt\")\n",
    "        model.fuse()\n",
    "    \n",
    "        return model\n",
    "\n",
    "\n",
    "    def predict(self, frame):\n",
    "       \n",
    "        results = self.model(frame)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        \n",
    "        xyxys = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "         # Extract detections for person class\n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            # class_id = boxes.cls[0]\n",
    "            # conf = boxes.conf[0]\n",
    "            # xyxy = boxes.xyxy[0]\n",
    "\n",
    "            if class_ids == 0.0:\n",
    "          \n",
    "              xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "              confidences.append(result.boxes.conf.cpu().numpy())\n",
    "              class_ids.append(result.boxes.cls.cpu().numpy().astype(int))\n",
    "            \n",
    "        \n",
    "        # Setup detections for visualization\n",
    "        detections = sv.Detections(\n",
    "                    xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "                    confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "                    class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n",
    "                    )\n",
    "        \n",
    "    \n",
    "        # Format custom labels\n",
    "        self.labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "        for _, confidence, class_id, tracker_id\n",
    "        in detections]\n",
    "        \n",
    "        # Annotate and display frame\n",
    "        frame = self.box_annotator.annotate(scene=frame, detections=detections, labels=self.labels)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __call__(self):\n",
    "\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "      \n",
    "        while True:\n",
    "          \n",
    "            start_time = time()\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "            \n",
    "            results = self.predict(frame)\n",
    "            frame = self.plot_bboxes(results, frame)\n",
    "            \n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time - start_time, 2)\n",
    "             \n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "            \n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    " \n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    \n",
    "detector = ObjectDetection(capture_index=0)\n",
    "detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x512 6 persons, 100.6ms\n",
      "Speed: 12.4ms preprocess, 100.6ms inference, 14.2ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "image = cv2.imread(\"image1_60_left.jpg\")\n",
    "model = YOLO(\"best_yolov8.pt\")\n",
    "results = model.predict(image)\n",
    "result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [[547.931640625, 696.5862426757812, 587.9410400390625, 804.6864624023438]] [[567.9363403320312, 750.6363525390625, 40.0093994140625, 108.1002197265625]]\n",
      "0.0 [[382.3617248535156, 712.8582153320312, 415.1719665527344, 812.1238403320312]] [[398.766845703125, 762.4910278320312, 32.81024169921875, 99.265625]]\n",
      "0.0 [[434.653076171875, 711.4818115234375, 481.1082458496094, 808.6229248046875]] [[457.88067626953125, 760.0523681640625, 46.455169677734375, 97.14111328125]]\n",
      "0.0 [[1136.162109375, 727.0494995117188, 1154.9908447265625, 772.88916015625]] [[1145.576416015625, 749.9693603515625, 18.8287353515625, 45.83966064453125]]\n",
      "0.0 [[483.3175964355469, 725.738525390625, 509.95855712890625, 803.1697387695312]] [[496.6380615234375, 764.4541015625, 26.640960693359375, 77.43121337890625]]\n",
      "0.0 [[509.769287109375, 729.0890502929688, 539.0842895507812, 803.2787475585938]] [[524.4267578125, 766.1838989257812, 29.31500244140625, 74.189697265625]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "box = []\n",
    "for i in range(len(result.boxes)):\n",
    "    box.append(result.boxes[i])\n",
    "    class_id, xyxy, xywh = box[i].cls.item() , box[i].xyxy.tolist(), box[i].xywh.tolist()\n",
    "    x1, y1, x2, y2 = xyxy[0]\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    print(class_id, xyxy, xywh)\n",
    "    image = cv2.rectangle(image, (x1, y1), (x2, y2), (random.randint(0,255), random.randint(0,255), random.randint(0,255)), 2)\n",
    "    image = cv2.putText(image, result.names[i], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[509.769287109375, 729.0890502929688, 539.0842895507812, 803.2787475585938]\n"
     ]
    }
   ],
   "source": [
    "xyxy= xyxy[0]\n",
    "print(xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x1,y1,x2,y2 \u001b[39m=\u001b[39m xyxy\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "x1,y1,x2,y2 = xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mrectangle(image, (x1, y1), (x2, y2), (random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m), random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m), random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m)), \u001b[39m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mputText(image, result\u001b[39m.\u001b[39mnames[i], (x1, y1), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m1\u001b[39m, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cv2.imshow('image',image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
